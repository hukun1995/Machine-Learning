[toc]

TensorFlow™ 是一个采用 数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。
TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。

TensorFlow的主要优点：
- 灵活性：支持底层数值计算，C++自定义操作符
- 可移植性：从服务器到PC到手机，从CPU到GPU到TPU
- 分布式计算：分布式并行计算，可指定操作符对应计算设备

俗话说，万丈高楼平地起，TensorFlow这座大厦也有它的地基。Tensorflow底层最核心的概念是张量，计算图以及自动微分。

### 一、张量
程序 = 数据结构+算法。
TensorFlow程序 = 张量数据结构 + 计算图算法语言

张量和计算图是 TensorFlow的核心概念。
Tensorflow的基本数据结构是张量Tensor。张量即多维数组。Tensorflow的张量和numpy中的array很类似。

从行为特性来看，有两种类型的张量，常量constant和变量Variable.
常量的值在计算图中不可以被重新赋值，变量可以在计算图中用assign等算子重新赋值。

#### 1.1 常量张量
张量的数据类型和numpy.array基本一一对应。

```python
i = tf.constant(1) # tf.int32 类型常量
l = tf.constant(1,dtype = tf.int64) # tf.int64 类型常量
f = tf.constant(1.23) #tf.float32 类型常量
d = tf.constant(3.14,dtype = tf.double) # tf.double 类型常量
s = tf.constant("hello world") # tf.string类型常量
b = tf.constant(True) #tf.bool类型常量


print(tf.int64 == np.int64) 
print(tf.bool == np.bool)
print(tf.double == np.float64)
print(tf.string == np.unicode_) # tf.string类型和np.unicode类型不等价
```
Output:
```
True
True
True
False
```

不同类型的数据可以用不同维度(rank)的张量来表示。标量为0维张量，向量为1维张量，矩阵为2维张量。可以简单地总结为：有几层中括号，就是多少维的张量。
- 通过tf.rank函数/.shape属性可以获得 张量的尺寸
- 通过tf.cast可以转化张量的数据类型
- 通过.numpy方法可以将张量转化为numpy数组
- tf和numpy一样可以进行向量级别的运算（+-*/等）
```python
#标量，0维张量
scalar = tf.constant(True)  
print('标量', scalar)
print(tf.rank(scalar))
print(scalar.shape)
print(scalar.numpy().ndim)  # tf.rank的作用和numpy的ndim方法相同

#向量，1维张量
vector = tf.constant([1.0,2.0,3.0,4.0]) 
f = tf.cast(vector, tf.int64)
print('向量', vector)
print(vector.dtype, f.dtype)
print(tf.rank(vector))
print(vector.shape)
print(np.ndim(vector.numpy()))

#矩阵, 2维张量
matrix = tf.constant([[1.0,2.0],[3.0,4.0]]) 
print('矩阵', matrix)
print(tf.rank(matrix).numpy())
print(matrix.shape)
print(np.ndim(matrix))

 # 3维张量
tensor3 = tf.constant([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]]) 
print('3维张量', matrix) 
print(tf.rank(tensor3))
print(tensor3.shape)
```
Output:
```
标量 tf.Tensor(True, shape=(), dtype=bool)
tf.Tensor(0, shape=(), dtype=int32)
()
0
向量 tf.Tensor([1. 2. 3. 4.], shape=(4,), dtype=float32)
<dtype: 'float32'> <dtype: 'int64'>
tf.Tensor(1, shape=(), dtype=int32)
(4,)
1
矩阵 tf.Tensor(
[[1. 2.]
 [3. 4.]], shape=(2, 2), dtype=float32)
2
(2, 2)
2
3维张量 tf.Tensor(
[[1. 2.]
 [3. 4.]], shape=(2, 2), dtype=float32)
tf.Tensor(3, shape=(), dtype=int32)
(2, 2, 2)
```

#### 1.2 变量张量

模型中需要被训练的参数一般被设置成变量。
```python
# 常量值不可以改变，常量的重新赋值相当于创造新的内存空间
c = tf.constant([1.0,2.0])
print(c)
print(id(c))
c = c + tf.constant([1.0,1.0])
print(c)
print(id(c))

print('-----分割线-----')

# 变量的值可以改变，可以通过assign, assign_add等方法给变量重新赋值
v = tf.Variable([1.0,2.0],name = "v")
print(v)
print(id(v))
v.assign_add([1.0,1.0])
print(v)
print(id(v))
```
Output:
```
tf.Tensor([1. 2.], shape=(2,), dtype=float32)
140363610406976
tf.Tensor([2. 3.], shape=(2,), dtype=float32)
140363610453616
-----分割线-----
<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>
140363608489024
<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>
140363608489024
```


### 二、计算图

三种计算图的构建方式：静态计算图，动态计算图，以及Autograph.

在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图。（类似求解器）
而在TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。

使用动态计算图即Eager Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。
使用动态计算图的缺点是运行效率相对会低一些。因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信。而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。

如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码。使用tf.function构建静态图的方式叫做 Autograph.

#### 2.1 计算图简介
计算图由节点(nodes)和线(edges)组成。
节点表示操作符Operator，或者称之为算子，线表示计算间的依赖。
实线表示有数据传递依赖，传递的数据即张量。虚线通常可以表示控制依赖，即执行先后顺序。

<img src="https://pic3.zhimg.com/80/v2-d377020f1d70c62b7463ca7c2eaa3912_1440w.jpg" width=500>


#### 2.2 静态计算图

**TensorFlow 1.0静态计算图范例**
在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。
```python
import tensorflow as tf

#定义计算图
g = tf.Graph()
with g.as_default():
    #placeholder为占位符，执行会话时候指定填充对象
    x = tf.placeholder(name='x', shape=[], dtype=tf.string)  
    y = tf.placeholder(name='y', shape=[], dtype=tf.string)
    z = tf.string_join([x,y],name = 'join',separator=' ')

#执行计算图
with tf.Session(graph = g) as sess:
    print(sess.run(fetches = z,feed_dict = {x:"hello",y:"world"}))
```

**TensorFlow2.0 怀旧版静态计算图**
TensorFlow2.0为了确保对老版本tensorflow项目的兼容性，在tf.compat.v1子模块中保留了对TensorFlow1.0那种静态计算图构建风格的支持。
可称之为怀旧版静态计算图，已经不推荐使用了。
```python
import tensorflow as tf

g = tf.compat.v1.Graph()
with g.as_default():
    x = tf.compat.v1.placeholder(name='x', shape=[], dtype=tf.string)
    y = tf.compat.v1.placeholder(name='y', shape=[], dtype=tf.string)
    z = tf.strings.join([x,y],name = "join",separator = " ")

with tf.compat.v1.Session(graph = g) as sess:
    # fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。
    result = sess.run(fetches = z,feed_dict = {x:"hello",y:"world"})
    print(result)
b'hello world'
```

#### 2.3 动态计算图
在TensorFlow2.0中，使用的是动态计算图和Autograph.
动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行。因此称之为**Eager Excution**.
Eager这个英文单词的原意是"迫不及待的"，也就是立即执行的意思。

```python
# 动态计算图在每个算子处都进行构建，构建后立即执行
x = tf.constant("hello")
y = tf.constant("world")
z = tf.strings.join([x,y],separator=" ")

tf.print(z)
```
Output:
```
hello world
```

#### 2.4 TensorFlow2.0的Autograph
动态计算图运行效率相对较低，可以用`@tf.function`装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。
在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。

不需要使用会话了，一些都像原始的Python语法一样自然。
实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用`@tf.function`切换成Autograph获得更高的效率。
当然，`@tf.function`的使用需要遵循一定的规范，我们后面章节将重点介绍。
```python
import tensorflow as tf

# 使用autograph构建静态图

@tf.function
def strjoin(x,y):
    z =  tf.strings.join([x,y],separator = " ")
    tf.print(z)
    return z

result = strjoin(tf.constant("hello"),tf.constant("world"))

print(result)
```
Output:
```
hello world
tf.Tensor(b'hello world', shape=(), dtype=string)
import datetime
```

### 三、自动积分

神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。而深度学习框架可以帮助我们自动地完成这种求梯度运算。
Tensorflow一般使用梯度磁带tf.GradientTape来记录正向运算过程，然后反播磁带自动得到梯度值。这种利用tf.GradientTape求微分的方法叫做Tensorflow的自动微分机制。

#### 3.1 利用梯度磁带求导数
**求一阶导**
```python
import tensorflow as tf
import numpy as np 


x = tf.Variable(0.0,name = "x",dtype = tf.float32)
a = tf.constant(1.0)
b = tf.constant(-2.0)
c = tf.constant(1.0)

# 对常量张量也可以求导，需要增加watch
with tf.GradientTape() as tape:
    # f(x) = a*x**2 + b*x + c
    tape.watch([a,b,c])
    y = a*tf.pow(x,2) + b*x + c
    
dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])
print(dy_dx)
print(dy_da)
print(dy_db)
print(dy_dc)
```
Output:
```
tf.Tensor(-2.0, shape=(), dtype=float32)
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(1.0, shape=(), dtype=float32)
```

**求二阶导**
```python
with tf.GradientTape() as tape2:
    with tf.GradientTape() as tape1:   
        y = a*tf.pow(x,2) + b*x + c
    dy_dx = tape1.gradient(y,x)   
    
dy2_dx2 = tape2.gradient(dy_dx,x)

print(dy2_dx2)
```
Output:
```
tf.Tensor(2.0, shape=(), dtype=float32)
```

#### 3.2 利用梯度磁带和优化器求最小值
**求导/梯度下降 分开**
```python
# 求f(x) = a*x**2 + b*x + c的最小值
# 使用optimizer.apply_gradients

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
a = tf.constant(1.0)
b = tf.constant(-2.0)
c = tf.constant(1.0)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
for _ in range(1000):
    with tf.GradientTape() as tape:
        y = a*tf.pow(x,2) + b*x + c
    dy_dx = tape.gradient(y,x)
    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])
    
tf.print("y =",y,"; x =",x)
```
Output:
```
y = 0 ; x = 0.999998569
```
**基于优化器一步到位**
```python
x = tf.Variable(0.0,name = "x",dtype = tf.float32)

#注意f()无参数
def f():   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    y = a*tf.pow(x,2)+b*x+c
    return(y)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   
for _ in range(1000):
    optimizer.minimize(f,[x])   
    
tf.print("y =",f(),"; x =",x)
```
Outut:
```
y = 0 ; x = 0.999998569
```

参考：
1. [30天吃掉tensorflow2](https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/2-1%2C%E5%BC%A0%E9%87%8F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md)
