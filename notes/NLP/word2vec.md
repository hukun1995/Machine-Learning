[toc]

### 1. 背景知识
Word2Vec是语言模型中的一种，它是从大量文本语料中以无监督方式学习语义知识的模型，被广泛地应用于自然语言处理中。

Word2Vec是用来生成词向量的工具，而词向量与语言模型有着密切的关系。因此，我们先来了解一些语言模型方面的知识。

#### 1.1 统计语言模型
统计语言模型是用来计算一个句子的概率的概率模型，它通常基于一个语料库来构建。那什么叫做一个句子的概率呢？假设`$W=(w_1, w_2,\dots,w_T)$`表示由T个词`$w_1,w_2,\dots,w_T$`按顺序构成的一个句子，则`$w_1,w_2,\dots,w_T$`的联合概率为：
```math
p(W)=p(w_1,w_2,\dots,w_T)
```

p(W)被称为语言模型，即用来计算这个句子概率的模型。利用Bayes公式，上式可以被链式地分解为：
```math
p(W) = p(w_1)p(w_2|w_1)p(w_3|w_1,w_2) \dots p(w_T|w_1,w_2\dots,w_{T-1}) \tag{1-1}
```

其中的(条件)概率`$p(w_1)p(w_2|w_1)\dotsp(w_T|w_1,w_2\dots,w_{T-1})$`就是语言模型的参数，若这些参数已经全部算得，那么给定一个句子，就可以很快地计算出相应地概率`$p(W)$`了。

看起来好像很简单，是吧？但是，具体实现起来还是有点麻烦。例如，先来看看模型参数的个数。刚才是考虑一个给定的长度为T的句子，就需要计算T个参数。不防假设语料库对应词典D的大小（即词汇量）为N，那么，如果考虑长度为T的任意句子，理论上就有`$N^T$`种可能，而每种可能都要计算T个参数，总共就需要计算`$TN^T$`个参数。当然，这里只是简单估算，并没有考虑重复参数，但这个量级还是有点吓人。此外，这些概率计算好后，还得保存下来，因此，存储这些信息也需要很大的内存开销。

此外，这些参数如何计算呢？常见的方法有n-gram模型、决策树、最大熵模型、最大熵马尔可夫模型、条件随机场、神经网络等方法。本文只讨论n-gram模型和神经网络两种方法。

#### 1.2 N-gram模型
考虑`$p(w_k|w_1,\dots,w_{k-1})$`的近似计算。利用Bayes&大数定理，当语料库足够大时有：
```math
p(w_k|w_1,\dots,w_{k-1}) = \frac{p(w_1,\dots,w_k)}{p(w_1,\dots,w_{k-1})} 
\approx \frac{count(w_1,\dots,w_k)}{count(w_1,\dots,w_{k-1})} 
\tag{1-2}
```
其中，count(.)表示词串在语料中出现的次数。可想而知，当k很大时count(.)的统计将会多么的耗时。

从公式（1-2）可以看出：一个词出现的概率与它前面的所有词都相关。如果假定一个词出现的概率只与它前面固定数目的词相关呢？这就是n-gram模型的基本思想，它做了一个n-1阶的Markov假设，认为一个词出现的概率就只与它前面的n-1个词相关，即，
```math
p(w_k|w_1,\dots,w_{k-1} \approx p(w_k|w_{k-n+1},\dots,w_{k-1}
```
于是，公式（2）就变成了
```math
p(w_k|w_1,\dots,w_{k-1}) = \frac{count(w_{k-n+1},\dots,w_k)}{count(w_{k-n+1},\dots,w_{k-1})} \tag{1-3}
```
这样简化，不仅使得单个参数的统计变得更容易（统计时需要匹配的词串更短），也使得参数的总数变少了。(原本参数p(d|a,b,c)和p(d|a,c,b)都需要，对于n=2时，只需要参数p(d|a)，相当于`$TN^T$`中的变成了2)

那么，n-gram中的参数取多大比较合适呢？一般来说，n的选取需要同时考虑计算复杂度和模型效果两个因素。

**表1-1：模型参数数量与n的关系**(参考`$TN^T$`进行复杂度理解)
n | 模型参数的数量
---|---
1（unigram）| `$2*10^5$`
2（bigram）| `$4*10^10$`
3（trigram）| `$8*10^15$`
4（4-gram）| `$16*10^20$`

在计算复杂度方面，表1-1给出了n-gram模型中模型参数数量随着的逐渐增大而变化的情况，其中假定词典大小N=200000(汉语的词汇量大大致是这个量级)。事实上，模型参数的量级是N的指数函数`$O(N^n)$`，显然n不能取得太大，实际应用中最多是采用的n=3三元模型。

在模型效果方面，理论上是越大，效果越好。现如今，互联网的海量数据以及机器性能的提升使得计算更高阶的语言模型（如n>10）成为可能，但需要注意的是，当n大到一定程度时，模型效果的提升幅度会变小。（具体可以参考吴军在《数学之美》中的相关章节）。事实上，这里还涉及到一个可靠性和可区别性的问题，参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性，因此需要在可靠性和可区别性之间进行折中。

另外，n-gram模型中还有一个叫做平滑化的重要环节（类似朴素贝叶斯）。这里不展开讨论。

总结起来，n-gram模型是这样一种模型，其主要工作是在语料中统计各种词串出现的次数以及平滑化处理。概率值计算好之后就存储起来，下次需要计算一个句子的概率时，只需找到相关的概率参数，将它们连乘起来就好了。

然而，在机器学习领域有一种通用的解决问题的方法：对所考虑的问题建模后先为其构造一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，最后利用这组最优参数对应的模型来进行预测。
对于统计语言模型而言，利用最大似然，可把目标函数设为：
```math
\prod_{w \in C} p(w|Context(w))
```
其中，C表示语料(Corpus)，Context(w)表示词的上下文，即周边的词的集合。当Context(w)为空时，就取`$P(w|Context(w))=P(w)$`。特别地，对于前面介绍的n-gram模型，就有 `$Context(w_i=w_{i-n+1},\dots,w_{i-1})$`。

当然，实际应用中常采用最大对数似然，即把目标函数设为
```math
L= \sum{w \in C} log p(w|Context(w)) \tag{1-4}
```

然后对这个函数进行最大化。
从公式（1-4）可见，概率`$p(w|Context(w))$`已被视为关于w和Context(w)的函数，即：
```math
p(w|Context(w)) = F(w, Context(w,) \theta)
```
其中`$\theta$`为待定参数集。这样一来，一旦对公式（1-4）进行优化得到最优参数集`$\theta^*$`后，F也就唯一被确定了，以后任何概率`$p(w|Context(w))$`就可以通过函数F来计算了。与n-gram相比，这种方法不需要事先计算并保存所有的概率值，（而是学习概率公式）通过直接计算来获取，且通选取合适的模型可使得`$\theta$`中参数的个数远小于n-gram中模型参数的个数。

很显然，对于这样一种方法，最关键的地方就在于函数F的构建了。下一小节将介绍一种通过神经网络来构造F的方法。之所以特意介绍这个方法，是因为它可以视为Word2Vec中算法框架的前身或者说基础。

#### 1.3 神经概率语言模型
本小节介绍 Bengio 等人于2003年在论文《A Neural Probabilistic Language Model》中提出的一种神经概率语言模型。该论文首次提出用神经网络来解决语言模型的问题，虽然在当时并没有得到太多的重视，却为后来深度学习在解决语言模型问题甚至很多别的NLP问题时奠定了坚实的基础，后人站在Yoshua Bengio的肩膀上，做出了更多的成就。包括Word2Vec的作者Tomas Mikolov在NNLM的基础上提出了RNNLM和后来的Word2Vec。文中也较早地提出将word表示一个低秩的向量，而不是One-Hot。word embedding作为一个language model的副产品，在后面的研究中起到了关键作用，为研究者提供了更加宽广的思路。值得注意的是Word2Vec的概念也是在该论文中提出的。

什么是词向量呢？简单来说就是，对词典中的任意词，指定一个固定长度的实值向量  ,  就称为  的词向量，为词向量的长度。关于词向量的进一步理解将放到下一节来讲解。

既然是神经概率语言模型，其中当然要用到神经网络了。 下图给出了神经网络的结构示意图。模型一共三层，第一层是映射层，将个单词映射为对应word embeddings的拼接，其实这一层就是MLP的输入层；第二层是隐藏层，激活函数用；第三层是输出层，因为是语言模型，需要根据前个单词预测下一个单词，所以是一个多分类器，用。整个模型最大的计算量集中在最后一层上，因为一般来说词汇表都很大，需要计算每个单词的条件概率，是整个模型的计算瓶颈。

![图片](https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuye04wHtHicTYNpVAGov28V130CXS0FFUeVdwRNCo31TT6xwFueFdkFJe78pvHuXdhLhaiaeicw1OWuA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

参考：
1. [深入浅出Word2Vec原理解析](https://mp.weixin.qq.com/s/zDneR1BU6xvt8cndEF4_Xw)
