[toc]

### 1. 背景知识
Word2Vec是语言模型中的一种，它是从大量文本语料中以无监督方式学习语义知识的模型，被广泛地应用于自然语言处理中。

Word2Vec是用来生成词向量的工具，而词向量与语言模型有着密切的关系。因此，我们先来了解一些语言模型方面的知识。

#### 1.1 统计语言模型
统计语言模型是用来计算一个句子的概率的概率模型，它通常基于一个语料库来构建。那什么叫做一个句子的概率呢？假设`$W=(w_1, w_2,\dots,w_T)$`表示由T个词`$w_1,w_2,\dots,w_T$`按顺序构成的一个句子，则`$w_1,w_2,\dots,w_T$`的联合概率为：
```math
P(W)=P(w_1,w_2,\dots,w_T)
```

P(W)被称为语言模型，即用来计算这个句子概率的模型。利用Bayes公式，上式可以被链式地分解为：
```math
P(W) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\dotsP(w_T|w_1,w_2\dots,w_{T-1}) \tag{1-1}
```

其中的(条件)概率`$P(w_1)P(w_2|w_1)\dotsP(w_T|w_1,w_2\dots,w_{T-1})$`就是语言模型的参数，若这些参数已经全部算得，那么给定一个句子，就可以很快地计算出相应地概率`$P(W)$`了。

看起来好像很简单，是吧？但是，具体实现起来还是有点麻烦。例如，先来看看模型参数的个数。刚才是考虑一个给定的长度为T的句子，就需要计算T个参数。不防假设语料库对应词典D的大小（即词汇量）为N，那么，如果考虑长度为T的任意句子，理论上就有`$N^T$`种可能，而每种可能都要计算T个参数，总共就需要计算`$TN^T$`个参数。当然，这里只是简单估算，并没有考虑重复参数，但这个量级还是有点吓人。此外，这些概率计算好后，还得保存下来，因此，存储这些信息也需要很大的内存开销。

此外，这些参数如何计算呢？常见的方法有n-gram模型、决策树、最大熵模型、最大熵马尔可夫模型、条件随机场、神经网络等方法。本文只讨论n-gram模型和神经网络两种方法。

#### 1.2 N-gram模型
考虑`$P(w_k|w_1,\dots,w_{k-1})$`的近似计算。利用Bayes&大数定理，当语料库足够大时有：
```math
P(w_k|w_1,\dots,w_{k-1}) = \frac{P(w_1,\dots,w_k)}{P(w_1,\dots,w_{k-1})} 
\approx \frac{count(w_1,\dots,w_k)}{count(w_1,\dots,w_{k-1})} 
\tag{1-2}
```
其中，count(.)表示词串在语料中出现的次数。可想而知，当k很大时count(.)的统计将会多么的耗时。

从公式（1-2）可以看出：一个词出现的概率与它前面的所有词都相关。如果假定一个词出现的概率只与它前面固定数目的词相关呢？这就是n-gram模型的基本思想，它做了一个n-1阶的Markov假设，认为一个词出现的概率就只与它前面的n-1个词相关，即，
```math
P(w_k|w_1,\dots,w_{k-1} \approx P(w_k|w_{k-n+1},\dots,w_{k-1}
```
于是，公式（2）就变成了
```math
P(w_k|w_1,\dots,w_{k-1}) = \frac{count(w_{k-n+1},\dots,w_k)}{count(w_{k-n+1},\dots,w_{k-1})}
```
这样简化，不仅使得单个参数的统计变得更容易（统计时需要匹配的词串更短），也使得参数的总数变少了。(原本参数P(d|a,b,c)和P(d|a,c,b)都需要，对于n=2时，只需要参数P(d|a)，相当于`$TN^T$`中的变成了2)

那么，n-gram中的参数取多大比较合适呢？一般来说，n的选取需要同时考虑计算复杂度和模型效果两个因素。

**表1-1：模型参数数量与n的关系**(参考`$TN^T$`进行复杂度理解)
n | 模型参数的数量
---|---
1（unigram）| `$2*10^5$`
2（bigram）| `$4*10^10$`
3（trigram）| `$8*10^15$`
4（4-gram）| `$16*10^20$`

在计算复杂度方面，表1-1给出了n-gram模型中模型参数数量随着的逐渐增大而变化的情况，其中假定词典大小N=200000(汉语的词汇量大大致是这个量级)。事实上，模型参数的量级是N的指数函数`$O(N^n)$`，显然n不能取得太大，实际应用中最多是采用的n=3三元模型。

参考：
1. [深入浅出Word2Vec原理解析](https://mp.weixin.qq.com/s/zDneR1BU6xvt8cndEF4_Xw)
