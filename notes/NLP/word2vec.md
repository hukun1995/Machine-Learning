[toc]

### 一、 背景知识
Word2Vec是语言模型中的一种，它是从大量文本语料中以无监督方式学习语义知识的模型，被广泛地应用于自然语言处理中。

Word2Vec是用来生成词向量的工具，而词向量与语言模型有着密切的关系。因此，我们先来了解一些语言模型方面的知识。

#### 1.1 统计语言模型
统计语言模型是用来计算一个句子的概率的概率模型，它通常基于一个语料库来构建。那什么叫做一个句子的概率呢？假设`$W=(w_1, w_2,\dots,w_T)$`表示由T个词`$w_1,w_2,\dots,w_T$`按顺序构成的一个句子，则`$w_1,w_2,\dots,w_T$`的联合概率为：
```math
p(W)=p(w_1,w_2,\dots,w_T)
```

p(W)被称为语言模型，即用来计算这个句子概率的模型。利用Bayes公式，上式可以被链式地分解为：
```math
p(W) = p(w_1)p(w_2|w_1)p(w_3|w_1,w_2) \dots p(w_T|w_1,w_2\dots,w_{T-1}) \tag{1-1}
```
其中的(条件)概率`$p(w_1)p(w_2|w_1)\dots p(w_T|w_1,w_2\dots,w_{T-1})$`就是语言模型的参数，若这些参数已经全部算得，那么给定一个句子，就可以很快地计算出相应地概率`$p(W)$`了。

看起来好像很简单，是吧？但是，具体实现起来还是有点麻烦。例如，先来看看模型参数的个数。刚才是考虑一个给定的长度为T的句子，就需要计算T个参数。不防假设语料库对应词典D的大小（即词汇量）为N，那么，如果考虑长度为T的任意句子，理论上就有`$N^T$`种可能，而每种可能都要计算T个参数，总共就需要计算`$TN^T$`个参数。当然，这里只是简单估算，并没有考虑重复参数，但这个量级还是有点吓人。此外，这些概率计算好后，还得保存下来，因此，存储这些信息也需要很大的内存开销。

此外，这些参数如何计算呢？常见的方法有n-gram模型、决策树、最大熵模型、最大熵马尔可夫模型、条件随机场、神经网络等方法。本文只讨论n-gram模型和神经网络两种方法。

#### 1.2 N-gram模型
考虑`$p(w_k|w_1,\dots,w_{k-1})$`的近似计算。利用Bayes&大数定理，当语料库足够大时有：
```math
p(w_k|w_1,\dots,w_{k-1}) = \frac{p(w_1,\dots,w_k)}{p(w_1,\dots,w_{k-1})} 
\approx \frac{count(w_1,\dots,w_k)}{count(w_1,\dots,w_{k-1})} 
\tag{1-2}
```
其中，count(.)表示词串在语料中出现的次数。可想而知，当k很大时count(.)的统计将会多么的耗时。

从公式（1-2）可以看出：一个词出现的概率与它前面的所有词都相关。如果假定一个词出现的概率只与它前面固定数目的词相关呢？这就是n-gram模型的基本思想，它做了一个n-1阶的Markov假设，认为一个词出现的概率就只与它前面的n-1个词相关，即，
```math
p(w_k|w_1,\dots,w_{k-1} \approx p(w_k|w_{k-n+1},\dots,w_{k-1}
```
于是，公式（2）就变成了
```math
p(w_k|w_1,\dots,w_{k-1}) = \frac{count(w_{k-n+1},\dots,w_k)}{count(w_{k-n+1},\dots,w_{k-1})} \tag{1-3}
```
这样简化，不仅使得单个参数的统计变得更容易（统计时需要匹配的词串更短），也使得参数的总数变少了。(原本参数p(d|a,b,c)和p(d|a,c,b)都需要，对于n=2时，只需要参数p(d|a)，相当于`$TN^T$`中的变成了2)

那么，n-gram中的参数取多大比较合适呢？一般来说，n的选取需要同时考虑计算复杂度和模型效果两个因素。

**表1-1：模型参数数量与n的关系**(参考`$TN^T$`进行复杂度理解)
n | 模型参数的数量
---|---
1（unigram）| `$2*10^5$`
2（bigram）| `$4*10^10$`
3（trigram）| `$8*10^15$`
4（4-gram）| `$16*10^20$`

在计算复杂度方面，表1-1给出了n-gram模型中模型参数数量随着的逐渐增大而变化的情况，其中假定词典大小N=200000(汉语的词汇量大大致是这个量级)。事实上，模型参数的量级是N的指数函数`$O(N^n)$`，显然n不能取得太大，实际应用中最多是采用的n=3三元模型。

在模型效果方面，理论上是越大，效果越好。现如今，互联网的海量数据以及机器性能的提升使得计算更高阶的语言模型（如n>10）成为可能，但需要注意的是，当n大到一定程度时，模型效果的提升幅度会变小。（具体可以参考吴军在《数学之美》中的相关章节）。事实上，这里还涉及到一个可靠性和可区别性的问题，++参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性，因此需要在可靠性和可区别性之间进行折中++。

另外，n-gram模型中还有一个叫做平滑化的重要环节（类似朴素贝叶斯）。这里不展开讨论。

总结起来，n-gram模型是这样一种模型，其主要工作是在语料中统计各种词串出现的次数以及平滑化处理。概率值计算好之后就存储起来，下次需要计算一个句子的概率时，只需找到相关的概率参数，将它们连乘起来就好了。

然而，在机器学习领域有一种通用的解决问题的方法：对所考虑的问题建模后先为其构造一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，最后利用这组最优参数对应的模型来进行预测。
对于统计语言模型而言，利用最大似然，可把目标函数设为：
```math
\prod_{w \in C} p(w|Context(w))
```
其中，C表示语料(Corpus)，Context(w)表示词的上下文，即周边的词的集合。当Context(w)为空时，就取`$P(w|Context(w))=P(w)$`。特别地，对于前面介绍的n-gram模型，就有 `$Context(w_i=w_{i-n+1},\dots,w_{i-1})$`。

当然，实际应用中常采用最大对数似然，即把目标函数设为
```math
L= \sum{w \in C} log p(w|Context(w)) \tag{1-4}
```

然后对这个函数进行最大化。
从公式（1-4）可见，概率`$p(w|Context(w))$`已被视为关于w和Context(w)的函数，即：
```math
p(w|Context(w)) = F(w, Context(w,) \theta)
```
其中`$\theta$`为待定参数集。这样一来，一旦对公式（1-4）进行优化得到最优参数集`$\theta^*$`后，F也就唯一被确定了，以后任何概率`$p(w|Context(w))$`就可以通过函数F来计算了。与n-gram相比，这种方法不需要事先计算并保存所有的概率值，（而是学习概率公式）通过直接计算来获取，且通选取合适的模型可使得`$\theta$`中参数的个数远小于n-gram中模型参数的个数。

很显然，对于这样一种方法，最关键的地方就在于函数F的构建了。下一小节将介绍一种通过神经网络来构造F的方法。之所以特意介绍这个方法，是因为它可以视为Word2Vec中算法框架的前身或者说基础。

#### 1.3 神经概率语言模型(NNLM)
本小节介绍 Bengio 等人于2003年在论文《A Neural Probabilistic Language Model》中提出的一种神经概率语言模型。该论文首次提出用神经网络来解决语言模型的问题，虽然在当时并没有得到太多的重视，却为后来深度学习在解决语言模型问题甚至很多别的NLP问题时奠定了坚实的基础，后人站在Yoshua Bengio的肩膀上，做出了更多的成就。包括Word2Vec的作者Tomas Mikolov在NNLM的基础上提出了RNNLM和后来的Word2Vec。文中也较早地提出将word表示一个低秩的向量，而不是One-Hot。word embedding作为一个language model的副产品，在后面的研究中起到了关键作用，为研究者提供了更加宽广的思路。值得注意的是Word2Vec的概念也是在该论文中提出的。

什么是词向量呢？简单来说就是，对词典D中的任意词w，指定一个固定长度的实值向量`$v(w) \in R^m$`, v(w)就称为  w的词向量，m为词向量的长度。关于词向量的进一步理解将放到下一节来讲解。

既然是神经概率语言模型，其中当然要用到神经网络了。 下图给出了神经网络的结构示意图。模型一共三层，第一层是映射层，将n个单词映射为对应word embeddings的拼接，其实这一层就是MLP的输入层；第二层是隐藏层，激活函数用tanh；第三层是输出层，因为是语言模型，需要根据前n个单词预测下一个单词，所以是一个多分类器，用Softmax。整个模型最大的计算量集中在最后一层上，因为一般来说词汇表都很大，需要计算每个单词的条件概率，是整个模型的计算瓶颈。

![image.png](https://note.youdao.com/yws/res/54809/WEBRESOURCEab50f7b878f36158763c88eb06b0d80d)

这里，需要注意的是需要提前初始化一个word embedding矩阵，每一行表示一个单词的向量。词向量也是训练参数，在每次训练中进行更新。这里可以看出词向量是语言模型的一个副产物，因为语言模型本身的工作是为了估计给定的一句话有多像人类的话，但从后来的研究发现，语言模型成了一个非常好的工具。

Softmax是一个非常低效的处理方式，需要先计算每个单词的概率，并且还要计算指数，指数在计算机中都是用级数来近似的，计算复杂度很高，最后再做归一化处理。此后很多研究都针对这个问题进行了优化，比如层级softmax、softmax tree。

当然NNLM的效果在现在看来并不算什么，但对于后面的相关研究具有非常重要的意义。论文中的Future Work提到了用RNN来代替MLP作为模型可能会取得更好的效果，在后面Tomas Mikolov的博士论文中得到了验证，也就是后来的RNNLM。

与n-gram模型相比，神经概率语言模型有什么优势呢？主要有以下两点：

1. **词语之间的相似性可以通过词向量来体现**。
举例来说，如果某个（英语）语料中S1="A dog is running in the room"出现了10000次，而S2="A cat is running in the room" 只出现了1次。按照n-gram模型的做法，p(S1)肯定会远大于p(S2) 。但这两句话的唯一区别在与dog和cat，而这两个词无论是句法还是语义上都扮演了相同的角色，因此， p(S1)和p(S2)应该很接近才对。
事实上，由神经概率语言模型算得的p(S1)和p(S1)是大致相等的。原因在于：（1）在神经概率语言模型中假定了“相似的”的词对应的词向量也是相似的；（2）概率函数关于词向量是光滑的，即词向量中的一个小变化对概率的影响也只是一个小变化。
2. **基于词向量的模型自带平滑化功能**
由于经过了Softmax函数，`$p(w|Contex(w)) \in (0, 1)$` 不会为零，不再需要像n-gram那样进行额外处理了。

最后，我们回过头来想想，词向量在整个神经概率语言模型中扮演了什么角色呢？训练时，它是用来帮助构造目标函数的辅助参数，训练完成后，它也好像只是语言模型的一个副产品。但这个副产品可不能小觑，下一节将对其作进一步阐述。

### 二、词向量
自然语言处理相关任务中要将自然语言交给机器学习中的算法来处理，通常需要将语言数学化，因为机器不是人，机器只认数学符号。向量是人把自然界的东西抽象出来交给机器处理的东西，基本上可以说向量是人对机器输入的主要方式了。

词向量就是用来将语言中的词进行数学化的一种方式，顾名思义，词向量就是把一个词表示成一个向量。 我们都知道词在送到神经网络训练之前需要将其编码成数值变量，常见的编码方式有两种：One-Hot Representation 和 Distributed Representation。

#### 2.1 One-Hot Representation
一种最简单的词向量方式是One-Hot编码 ，就是用一个很长的向量来表示一个词，向量的长度为词典的大小，向量中只有一个1， 其他全为0，1 的位置对应该词在词典中的位置。

举个例子：I like writing code，那么转换成独热编码就是:

词	| One-Hot 编码
---|---
I |	1 0 0 0
like |	0 1 0 0
writing |	0 0 1 0
code |	0 0 0 1

这种词表示简单且易理解，应对一些简单的NLP问题也确实可行。但它存在三个缺点：
1. **容易受维数灾难的困扰，尤其是将其用于 Deep Learning的一些算法时**
随着词典中词数量的激增，词向量的长度也将激增，所占据的内存和计算资源都将难以承受。
2. **词汇鸿沟，不能很好地刻画词与词之间的相似性**
任意两个词之间都是孤立的，从这两个向量中看不出两个词是否有关系。比如说，I、like之间的关系和like、writing之间的关系，无论通过向量的距离还是1的位置都很难表现，你会发现独热编码完全没法表现单词之间的任何关系。
3. **强稀疏性**
当维度过度增长的时候,你会发现0特别多,这样造成的后果就是整个向量中有用的信息特别少,几乎就没法做计算。

由于One-hot编码存在以上种种问题，所以研究者就会寻求发展，用另外的方式表示，就是Distributed Representation。

#### 2.2 Distributed Representation
Distributed Representation最早是Hinton于1986年提出的，可以克服One-Hot Representation的上述缺点。其基本想法是：通过训练将某种语言中的每一个词 映射成一个固定长度的短向量（当然这里的“短”是相对于One-Hot Representation的“长”而言的），所有这些向量构成一个词向量空间，而每一个向量则可视为 该空间中的一个点，在这个空间上引入“距离”，就可以根据词之间的距离来判断它们之间的语法、语义上的相似性了。Word2Vec中采用的就是这种Distributed Representation 的词向量。

为什么叫做 Distributed Representation？很多人问到这个问题。一个简单的解释是这样的：对于One-Hot Representation，向量中只有一个非零分量，非常集中（有点孤注一掷的感觉）；而对于Distributed Representation，向量中有大量非零分量，相对分散（有点风险平摊的感觉），把词的信息分布到各个分量 中去了。这一点，跟并行计算里的分布式并行很像。

如何获取词向量呢？有很多不同模型可以用来估计词向量，包括有名的LSA（Latent Semantic Analysis）和LDA（Latent Dirichlet Allocation）。此外，利用神经 网络算法也是一种常用的方法，上一节介绍的神经概率语言模型就是一个很好的实例。当然，在那个模型中，目标是生成语言模型，词向量只是一个副产品。事实上， 大部分情况下，词向量和语言模型都是捆绑在一起的，训练完成后两者同时得到。在用神经网络训练语言模型方面，最经典的论文就是Bengio于2003年发表的《A Neural Probabilistic Language Model》 ，其后有一系列相关的研究工作，其中也包括谷歌Tomas Mikolov团队的Word2Vec。

参考：
1. [深入浅出Word2Vec原理解析](https://mp.weixin.qq.com/s/zDneR1BU6xvt8cndEF4_Xw)
